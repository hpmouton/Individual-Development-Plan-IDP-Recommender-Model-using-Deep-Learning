\chapter{Algorithms}
\label{appendix:algorithms}

This appendix contains an example algorithm.
\setlength{\textfloatsep}{6pt}

\newcommand{\hh}{\hspace*{3pt}}

\SetKwInput{KwInit}{Init}
\SetKwInput{KwHyper}{Hyperparameters}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Principal Component Analysis (PCA)}
\label{alg:pca}

\KwIn{Dataset $\mathbf{X} \in \mathbb{R}^{n \times d}$ (n samples, d features)}
\KwOut{Reduced representation $\mathbf{X}_{\text{reduced}}$}

\Begin{
    Center the data: $\mathbf{X} \leftarrow \mathbf{X} - \text{mean}(\mathbf{X})$\;
    Compute covariance matrix: $\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}$\;
    Compute eigenvectors and eigenvalues of $\mathbf{C}$\;
    Sort eigenvectors by descending eigenvalues\;
    Select top $k$ eigenvectors to form matrix $\mathbf{W}$\;
    Project data: $\mathbf{X}_{\text{reduced}} = \mathbf{X} \mathbf{W}$
}
\end{algorithm}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Autoencoder Training for Feature Extraction}
\label{alg:autoencoder}

\KwIn{Training data $\mathbf{X}$}
\KwOut{Encoded representation $\mathbf{Z}$}

\Begin{
    Define encoder network $f_{\theta}$ and decoder network $g_{\phi}$\;
    Initialize parameters $\theta, \phi$\;
    \For{each epoch}{
        \For{each mini-batch}{
            Encode: $\mathbf{Z} = f_{\theta}(\mathbf{X})$\;
            Decode: $\hat{\mathbf{X}} = g_{\phi}(\mathbf{Z})$\;
            Compute reconstruction loss: $L = \|\mathbf{X} - \hat{\mathbf{X}}\|^2$\;
            Update $\theta, \phi$ using gradient descent to minimize $L$\;
        }
    }
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Neural Collaborative Filtering (NCF)}
\label{alg:ncf}

\KwIn{User-item interaction matrix $\mathbf{R}$}
\KwOut{Predicted user preferences $\hat{\mathbf{R}}$}

\Begin{
    Initialize embeddings for users and items\;
    Define neural network $f(\cdot)$ to model interaction between embeddings\;
    \For{each epoch}{
        \For{each user-item pair $(u, i)$}{
            Predict score: $\hat{r}_{ui} = f(\text{embedding}(u), \text{embedding}(i))$\;
            Compute loss: $L = \text{MSE}(r_{ui}, \hat{r}_{ui})$\;
            Update model parameters via backpropagation\;
        }
    }
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Graph Neural Network (GNN) for Skill Recommendation}
\label{alg:gnn}

\KwIn{Graph $G = (V, E)$ with node features $\mathbf{H}^0$}
\KwOut{Updated node embeddings $\mathbf{H}^L$}

\Begin{
    \For{layer $l = 0$ \KwTo $L-1$}{
        \For{each node $v \in V$}{
            Aggregate neighbor features:
            
            $\mathbf{h}_{\mathcal{N}(v)}^{(l)} = \text{AGGREGATE}^{(l)}(\{\mathbf{h}_u^{(l)}: u \in \mathcal{N}(v)\})$\;
            
            Update node representation:
            
            $\mathbf{h}_v^{(l+1)} = \sigma\left( \mathbf{W}^{(l)} \cdot \text{CONCAT}(\mathbf{h}_v^{(l)}, \mathbf{h}_{\mathcal{N}(v)}^{(l)}) \right)$\;
        }
    }
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Transformer Encoder for Text Embeddings}
\label{alg:transformer}

\KwIn{Tokenized text input $X = (x_1, \ldots, x_n)$}
\KwOut{Contextual embeddings $H = (h_1, \ldots, h_n)$}

\Begin{
    Embed tokens: $\mathbf{E} = \text{Embedding}(X)$\;
    Add positional encodings to $\mathbf{E}$\;
    \For{each layer}{
        Compute multi-head self-attention:

        $Z = \text{MultiHead}(Q=E, K=E, V=E)$\;
        
        Apply feedforward network: 
        
        $H = \text{FFN}(Z)$\;
    }
}
\end{algorithm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Deep Learning-based IDP Recommender System Pipeline}
\label{alg:idp_pipeline}

\KwIn{Raw employee data: skills assessments, job descriptions, performance reviews, career goals}
\KwOut{Personalized Individual Development Plans (IDPs)}

\Begin{
    \textbf{Step 1: Data Collection and Preprocessing}\;
    \Indp
    Collect employee datasets (skills, evaluations, feedback, goals)\;
    Clean and normalize data (handling missing values, text cleaning)\;
    Extract features from structured data and unstructured text using NLP\;
    \Indm

    \textbf{Step 2: Feature Engineering}\;
    \Indp
    Apply PCA for dimensionality reduction\;
    Train Autoencoder to learn compact feature representations\;
    Perform Clustering to identify similar employee profiles\;
    \Indm

    \textbf{Step 3: Model Training}\;
    \Indp
    \textbf{Sub-step 3.1: NCF-based Interaction Modeling}\;
    \Indp
    Train Neural Collaborative Filtering (NCF) to predict personalized recommendations based on historical data\;
    \Indm
    \textbf{Sub-step 3.2: Graph Modeling}\;
    \Indp
    Construct Knowledge Graph of skills, learning paths, career objectives\;
    Train Graph Neural Network (GNN) to capture relational dependencies\;
    \Indm
    \textbf{Sub-step 3.3: Textual Understanding}\;
    \Indp
    Train Transformer model (e.g., BERT) on employee goal statements\;
    Generate embeddings for career goal descriptions\;
    \Indm
    \Indm

    \textbf{Step 4: Inference and Recommendation}\;
    \Indp
    For a given employee:
    \begin{itemize}
        \item Embed structured and unstructured data
        \item Aggregate predictions from NCF, GNN, and Transformer models
        \item Rank and select top development plan recommendations
    \end{itemize}
    Generate customized IDP output for the employee\;
    \Indm

    \textbf{Step 5: Feedback Integration}\;
    \Indp
    Collect employee and supervisor feedback post-recommendation\;
    Update training datasets and retrain models periodically for dynamic adaptation\;
    \Indm
}
\end{algorithm}
